{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOIz1xN1pelT",
        "outputId": "c0539505-9b33-4b7b-fa79-f740b70e2ade"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Huh6rlQEotZv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v158s3JLotZv"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dwYTNCDsotZw"
      },
      "outputs": [],
      "source": [
        "max_length = 128  # Maximum length of input sentence to the model.\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEWYU0KIotZw"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WM_V_f3lotZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00cf5b1-b9a3-4d49-ef44-862e5d89daeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11.1M  100 11.1M    0     0  11.9M      0 --:--:-- --:--:-- --:--:-- 11.9M\n",
            "SNLI_Corpus/\n",
            "SNLI_Corpus/snli_1.0_dev.csv\n",
            "SNLI_Corpus/snli_1.0_train.csv\n",
            "SNLI_Corpus/snli_1.0_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Using SNLI dataset with 3 classes\n",
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RKm5ItpfotZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab84565-ec26-49e5-f8a7-e8ba81e76568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train samples : 1000\n",
            "Total validation samples: 100\n",
            "Total test samples: 100\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=1000)\n",
        "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\", nrows=100)\n",
        "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\", nrows=100)\n",
        "\n",
        "print(f\"Total train samples : {train_df.shape[0]}\")\n",
        "print(f\"Total validation samples: {valid_df.shape[0]}\")\n",
        "print(f\"Total test samples: {valid_df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i6rp-lPhotZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c52d53-ee27-4486-fbd1-10dc88fc4f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence1: A person on a horse jumps over a broken down airplane.\n",
            "Sentence2: A person is at a diner, ordering an omelette.\n",
            "Similarity: contradiction\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\n",
        "print(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\n",
        "print(f\"Similarity: {train_df.loc[1, 'similarity']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5nXinjLotZz"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gk_rrotxotZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f26b00-8f1c-441d-f009-ef08caa5cd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values\n",
            "similarity    0\n",
            "sentence1     0\n",
            "sentence2     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Dropping NaN entries in train data\n",
        "print(\"Number of missing values\")\n",
        "print(train_df.isnull().sum())\n",
        "train_df.dropna(axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_vJ-xnfXotZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790c41b7-ed73-443f-988e-34518a7c96ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Target Distribution\n",
            "entailment       334\n",
            "neutral          332\n",
            "contradiction    332\n",
            "-                  2\n",
            "Name: similarity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Target Distribution\")\n",
        "print(train_df.similarity.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JwqzfrCZotZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80c54fd-ddfc-4594-b457-65050b859528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Target Distribution\n",
            "contradiction    36\n",
            "entailment       32\n",
            "neutral          31\n",
            "-                 1\n",
            "Name: similarity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Validation Target Distribution\")\n",
        "print(valid_df.similarity.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jMsEHueJotZ2"
      },
      "outputs": [],
      "source": [
        "# Skipping \"-\" samples\n",
        "train_df = (\n",
        "    train_df[train_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "valid_df = (\n",
        "    valid_df[valid_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CzIXMbEDotZ2"
      },
      "outputs": [],
      "source": [
        "# One-hot encode training, validation, and test labels.\n",
        "\n",
        "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n",
        "\n",
        "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n",
        "\n",
        "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OcpBPKa5otZ3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates batches of data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "        # encoded together and separated by [SEP] token.\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "        if self.shuffle:\n",
        "            np.random.RandomState(42).shuffle(self.indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feo_8Lj6otZ3"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GQynp-89otZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d025f6f-0ce6-4b87-d1ec-5d3a140cdf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f0d997235d0>\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
            "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
            "                                n_state=(None, 128,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 128, 128)    426496      ['tf_bert_model_1[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['bidirectional_1[0][0]']        \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 128)         0           ['bidirectional_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_max_pooling1d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 256)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            771         ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,909,507\n",
            "Trainable params: 427,267\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create the model under a distribution strategy scope.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    # Encoded token ids from BERT tokenizer.\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "    )\n",
        "    # Attention masks indicates to the model which tokens should be attended to.\n",
        "    attention_masks = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "    )\n",
        "    # Token type ids are binary masks identifying different sequences in the model.\n",
        "    token_type_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "    )\n",
        "    # Loading pretrained BERT model.\n",
        "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_model.trainable = False\n",
        "    bert_output = bert_model(\n",
        "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "    )\n",
        "    sequence_output = bert_output.last_hidden_state\n",
        "    pooled_output = bert_output.pooler_output\n",
        "    \n",
        "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "    bi_lstm = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "    )(sequence_output)\n",
        "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"acc\"],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Strategy: {strategy}\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ASC-1gPXotZ4"
      },
      "outputs": [],
      "source": [
        "train_data = BertSemanticDataGenerator(\n",
        "    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "valid_data = BertSemanticDataGenerator(\n",
        "    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_val,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ojVtlPgcotZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e41ade-e375-43b7-e22d-283969ccb0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "31/31 [==============================] - ETA: 0s - loss: 1.1546 - acc: 0.3659 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 481s 15s/step - loss: 1.1546 - acc: 0.3659 - val_loss: 1.0424 - val_acc: 0.3854\n",
            "Epoch 2/2\n",
            "31/31 [==============================] - 459s 15s/step - loss: 1.0556 - acc: 0.4224 - val_loss: 0.9550 - val_acc: 0.6042\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=epochs,\n",
        "    use_multiprocessing=True,\n",
        "    workers=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpPzdvJYotZ4"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w8aD3NAYotZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c7a5e0-2895-4446-d3ae-f99a772ba3ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
            "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
            "                                n_state=(None, 128,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 128, 128)    426496      ['tf_bert_model_1[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['bidirectional_1[0][0]']        \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 128)         0           ['bidirectional_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_max_pooling1d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 256)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            771         ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,909,507\n",
            "Trainable params: 109,909,507\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Unfreeze the bert_model.\n",
        "bert_model.trainable = True\n",
        "# Recompile the model to make the change effective.\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfHuEky6otZ5"
      },
      "source": [
        "# Train the entire model end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YglY8pK_otZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4400ce26-1041-494a-95f4-d73930332c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "31/31 [==============================] - 1285s 41s/step - loss: 0.9568 - accuracy: 0.5403 - val_loss: 0.8496 - val_accuracy: 0.6562\n",
            "Epoch 2/2\n",
            "31/31 [==============================] - 1233s 40s/step - loss: 0.8467 - accuracy: 0.6280 - val_loss: 0.7849 - val_accuracy: 0.7292\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=epochs,\n",
        "    use_multiprocessing=True,\n",
        "    workers=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgDc5VqCotZ5"
      },
      "source": [
        "## Evaluate model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OaAcIdWzotZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd1c08f-e586-4118-e829-ce457eb097a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 36s 12s/step - loss: 0.8692 - accuracy: 0.6979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8691844940185547, 0.6979166865348816]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "test_data = BertSemanticDataGenerator(\n",
        "    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "model.evaluate(test_data, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2qhMbbkotZ5"
      },
      "source": [
        "## Cub-it's data test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "m87s9OG7otZ5"
      },
      "outputs": [],
      "source": [
        "def check_similarity(sentence1, sentence2):\n",
        "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "\n",
        "    proba = model.predict(test_data[0])[0]\n",
        "    idx = np.argmax(proba)\n",
        "    proba = f\"{proba[idx]: .2f}%\"\n",
        "    pred = labels[idx]\n",
        "    return pred, proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "v2GNNgz0otZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2793a289-cb08-435d-c72c-8ebed5aa4040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('entailment', ' 0.54%')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "sentence1 = \"Unable to connect to the terminal to perform test tasks, Is this due to the fact that some \" + \\\n",
        "            \"limited number of people can connect to the terminals, or is this a bug?\"\n",
        "sentence2 = \"It is not very clear how the C Programming Workshop differs from the Simulator for Problem Solving\"\n",
        "check_similarity(sentence1, sentence2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cubit_test_dataset = [\n",
        "                      {\"Unable to connect to the terminal to perform test tasks, Is this due to the fact that some limited number of people can connect to the terminals, or is this a bug?\", \n",
        "                       \"It is not very clear how the C Programming Workshop differs from the Simulator for Problem Solving\"},\n",
        "                      {\"Do you issue e-certificates after completing a course?\", \n",
        "                       \"Can I get a certificate upon completion of the course?\"},\n",
        "                      {\"The terminal does not open, what should I do in this case?\", \n",
        "                       \"Help please, terminal cannot be opened.\"},\n",
        "                      {\"The question is, is it possible to give an answer in the discussion?\", \n",
        "                       \"Can I get a certificate upon completion of the course?\"},\n",
        "                      {\"Is it possible to thank the creators without buying a course, but at the same time support this project?\", \n",
        "                       \"Is it possible to donate developers for good courses?\"},\n",
        "                      {\"Is it possible to thank the creators without buying a course, but at the same time support this project?\", \n",
        "                       \"The question is, is it possible to give an answer in the discussion?\"},\n",
        "                      {\"The terminal does not open, what should I do in this case?\", \n",
        "                       \"Can I get a certificate upon completion of the course?\"},\n",
        "                      {\"Do you issue e-certificates after completing a course?\", \n",
        "                       \"Is it possible to thank the creators without buying a course, but at the same time support this project?\"},\n",
        "                      {\"When I try to pass any task for programming on the course Programming in C, an error appears.\", \n",
        "                       \"Help please, terminal cannot be opened.\"},\n",
        "                      {\"Unable to complete the task due to an error while connecting to the terminal.\", \n",
        "                       \"The terminal does not open, what should I do in this case?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "zKztwexaK78w"
      },
      "execution_count": 63,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "QA_equality_proto",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}